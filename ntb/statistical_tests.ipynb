{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch_metrics_to_df(df):\n",
    "    # Find all rows with same name and concatenate, drop Nones\n",
    "    metrics_names = df.columns.unique()\n",
    "    metrics = {}\n",
    "    for metric_name in metrics_names:\n",
    "        numpy_array = np.concatenate(df[metric_name].values, axis=0)\n",
    "        # delete nans\n",
    "        numpy_array = numpy_array[~np.isnan(numpy_array)]\n",
    "        metrics[metric_name] = numpy_array\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"small-10-gen\"\n",
    "exp_ids_to_drop = [\"Y7IGw7GruwZud8VT\"]\n",
    "# Decide how to subset\n",
    "subset_keys = [\"data_params.name\"]\n",
    "sep_str_converter = {\"model.rec_model.name\": lambda x:x,\n",
    "                     \"init.method\": lambda x: x if (x is not None and isinstance(x,str)) else \"Random\",\n",
    "                     \"init.training.cutoff\": lambda x: \"cutoff\" if x is not None and x is True else \"\"}\n",
    "sep_keys = list(sep_str_converter.keys())\n",
    "metrics_to_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.rec_model.name', 'init.method', 'init.training.cutoff']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_folder = f\"../out/exp/{exp_name}\"\n",
    "log_folder = f\"../out/log/{exp_name}\"\n",
    "results_folder = f\"../out/results/{exp_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all re-run exp_ids of log_folder\n",
    "exp_logs = {}\n",
    "for exp_id in os.listdir(log_folder):\n",
    "    for version_str in os.listdir(f\"{log_folder}/{exp_id}/lightning_logs\"):\n",
    "        if version_str == \"version_1\":\n",
    "            exp_ids.append(exp_id)\n",
    "            exp_logs[exp_id] = convert_batch_metrics_to_df(pd.read_csv(f\"{log_folder}/{exp_id}/lightning_logs/version_1/metrics_per_sample.csv\", header=None, index_col=0).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration of each exp_id\n",
    "exp_confs = {}\n",
    "for exp_id in exp_ids:\n",
    "    if exp_id in exp_ids_to_drop: continue\n",
    "    exp_id_config_path = f\"{exp_folder}/{exp_id}.json\"\n",
    "    with open(exp_id_config_path, \"r\") as f:\n",
    "        exp_confs[exp_id] = flatten_dict(json.load(f))\n",
    "exp_confs_df = pd.DataFrame(exp_confs).T # concatenate exp_confs with exp_ids as row_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_function(prec, rec):\n",
    "    app = 2*prec*rec/(prec+rec)\n",
    "    app[np.isnan(app)] = 0\n",
    "    return app\n",
    "\n",
    "for exp_id,df in exp_logs.items():\n",
    "    for k in [5,10,20]:\n",
    "        exp_logs[exp_id][f\"test_F1_@{k}\"] = f1_function(df[f\"test_Precision_@{k}\"], df[f\"test_Recall_@{k}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id_to_conf_name = {}\n",
    "group_subsets = {}\n",
    "for group_subset_key_values, group_df in exp_confs_df.groupby(subset_keys):\n",
    "    group_ids = group_df.index\n",
    "    group_subsets[group_subset_key_values] = group_ids\n",
    "    for sep_values,sep_df in group_df.groupby(sep_keys, dropna=False):\n",
    "        if len(sep_df.index) > 1:\n",
    "            raise ValueError((f\"Multiple exps for {sep_values}\"))\n",
    "        conf_name = [*group_subset_key_values]\n",
    "        for sep_key,sep_values in zip(sep_keys,sep_values):\n",
    "            app = sep_str_converter[sep_key](sep_values)\n",
    "            if app != \"\" and (isinstance(app,str) or not np.isnan(app)): conf_name.append(app)\n",
    "        conf_name = \"_\".join(conf_name)\n",
    "        exp_id_to_conf_name[sep_df.index[0]] = conf_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_per_metric_per_exp_id = {}\n",
    "# for metric_name in metrics_to_test:\n",
    "#     for exp_id, df in exp_logs.items():\n",
    "#         avg_per_metric_per_exp_id[exp_id] = df[metric_name].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_test = [f\"test_{metric}_@{k}\" for metric in [\"F1\",\"NDCG\"] for k in [5,10,20]]\n",
    "stat_test = lambda x,y: np.round(wilcoxon(x, y, alternative=\"greater\").pvalue,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_tests = {}\n",
    "for group_subset_values, group_subset_exp_ids in group_subsets.items():\n",
    "    for exp_id1, exp_id2 in it.permutations(group_subset_exp_ids,2):\n",
    "        for metric_name in metrics_to_test:\n",
    "            arr1 = exp_logs[exp_id1][metric_name]\n",
    "            arr2 = exp_logs[exp_id2][metric_name]\n",
    "            pvalue = stat_test(arr1, arr2)\n",
    "            #stat_tests[(exp_id_to_conf_name[exp_id1], exp_id_to_conf_name[exp_id2], metric_name)] = pvalue\n",
    "            stat_tests[(exp_id_to_conf_name[exp_id1], exp_id_to_conf_name[exp_id2], metric_name)] = f\"{np.mean(arr1)} vs {np.mean(arr2)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if pvalue < alpha, arr1 is greater than arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stat tests in a csv\n",
    "stat_tests_df = pd.DataFrame(stat_tests, index=[\"pvalue\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">behance_BERT4Rec_svd</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">behance_BERT4Rec_leporid</th>\n",
       "      <th>test_F1_@5</th>\n",
       "      <td>0.16636029618344836 vs 0.17422385837259843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_F1_@10</th>\n",
       "      <td>0.11250835713906884 vs 0.1183990657750171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_F1_@20</th>\n",
       "      <td>0.07121702948874009 vs 0.07395979329983982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_NDCG_@5</th>\n",
       "      <td>0.3748891823194629 vs 0.38620753600672986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_NDCG_@10</th>\n",
       "      <td>0.4135545370210072 vs 0.42793945013545454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">bookcrossing_SASRec_svd</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bookcrossing_GRU4Rec_leporid</th>\n",
       "      <th>test_F1_@10</th>\n",
       "      <td>0.017094435968247064 vs 0.017290360735791443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_F1_@20</th>\n",
       "      <td>0.01785714311056394 vs 0.0176390601846447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_NDCG_@5</th>\n",
       "      <td>0.02528112373266626 vs 0.028195357114751022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_NDCG_@10</th>\n",
       "      <td>0.04127926222080815 vs 0.04304827090577576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_NDCG_@20</th>\n",
       "      <td>0.06455262009744886 vs 0.06548104682889093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                          pvalue\n",
       "behance_BERT4Rec_svd    behance_BERT4Rec_leporid     test_F1_@5       0.16636029618344836 vs 0.17422385837259843\n",
       "                                                     test_F1_@10       0.11250835713906884 vs 0.1183990657750171\n",
       "                                                     test_F1_@20      0.07121702948874009 vs 0.07395979329983982\n",
       "                                                     test_NDCG_@5      0.3748891823194629 vs 0.38620753600672986\n",
       "                                                     test_NDCG_@10     0.4135545370210072 vs 0.42793945013545454\n",
       "...                                                                                                          ...\n",
       "bookcrossing_SASRec_svd bookcrossing_GRU4Rec_leporid test_F1_@10    0.017094435968247064 vs 0.017290360735791443\n",
       "                                                     test_F1_@20       0.01785714311056394 vs 0.0176390601846447\n",
       "                                                     test_NDCG_@5    0.02528112373266626 vs 0.028195357114751022\n",
       "                                                     test_NDCG_@10    0.04127926222080815 vs 0.04304827090577576\n",
       "                                                     test_NDCG_@20    0.06455262009744886 vs 0.06548104682889093\n",
       "\n",
       "[444 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_tests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stat tests in a csv\n",
    "stat_tests_df.to_csv(f\"{results_folder}/prova_last_stat_tests_palue.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='../out/exp/small-10-gen/GUY0Cs8x6YG5GC20.json' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f#\"{results_folder}/stat_tests_means.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
